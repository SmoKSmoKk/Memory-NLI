{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239730ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nl2pandas.backend.nli_for_pandas.embedding.BERT import BERT\n",
    "from nl2pandas.backend.nli_for_pandas.similarity.cosine_similarity import CosineSimilarity\n",
    "from nl2pandas.backend.nli_for_pandas.data.data import Data\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01435d8d",
   "metadata": {},
   "source": [
    "calculate embedding for the different models from sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b973ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"paraphrase-mpnet-base-v2\",\n",
    "    \"paraphrase-TinyBERT-L6-v2\",\n",
    "    \"paraphrase-distilroberta-base-v2\",\n",
    "    \"paraphrase-MiniLM-L12-v2\",\n",
    "    \"paraphrase-MiniLM-L6-v2\",\n",
    "    \"paraphrase-albert-small-v2\",\n",
    "    \"paraphrase-MiniLM-L3-v2\",\n",
    "    \"nli-mpnet-base-v2\",\n",
    "    \"stsb-mpnet-base-v2\",\n",
    "    \"stsb-distilroberta-base-v2\",\n",
    "    \"nli-roberta-base-v2\",\n",
    "    \"stsb-roberta-base-v2\",\n",
    "    \"nli-distilroberta-base-v2\",\n",
    "]\n",
    "\n",
    "overall = [\n",
    "    76.84,\n",
    "    75.36,\n",
    "    75.15,\n",
    "    74.81,\n",
    "    74.38,\n",
    "    73.94,\n",
    "    73.55,\n",
    "    72.45,\n",
    "    72.12,\n",
    "    70.07,\n",
    "    70.00,\n",
    "    69.89,\n",
    "    69.86,\n",
    "    61.57,\n",
    "    60.52\n",
    "]\n",
    "\n",
    "bert_models = [BERT(model) for model in models]\n",
    "\n",
    "data = Data(file=\"./evaluation/big_action_set.csv\")\n",
    "data_small = Data(file=\"./evaluation/small_action_set.csv\")\n",
    "# add some totally unrelated utterances\n",
    "unrelated = [\n",
    "    \"give me an apple\",\n",
    "    \"please send your manager an e-mail\",\n",
    "    \"I really like hot dogs\",\n",
    "    \"bla bla blublibla\",\n",
    "    \"test test test test\"\n",
    "]\n",
    "for utterance in unrelated:\n",
    "    data.utterances.append(utterance)\n",
    "    data.actions.append(utterance)\n",
    "    data_small.utterances.append(utterance)\n",
    "    data_small.actions.append(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959bde10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paraphrase-mpnet-base-v2\n",
      "56.5 ms ± 1.23 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-TinyBERT-L6-v2\n",
      "31.2 ms ± 1.73 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-distilroberta-base-v2\n",
      "40.9 ms ± 1.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-MiniLM-L12-v2\n",
      "36.3 ms ± 578 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-MiniLM-L6-v2\n",
      "19.4 ms ± 243 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "paraphrase-albert-small-v2\n",
      "41.7 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-MiniLM-L3-v2\n",
      "9.52 ms ± 1.65 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "nli-mpnet-base-v2\n",
      "56 ms ± 1.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "stsb-mpnet-base-v2\n",
      "81.7 ms ± 1.59 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "stsb-distilroberta-base-v2\n",
      "40.8 ms ± 1.48 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "nli-roberta-base-v2\n",
      "85.8 ms ± 5.78 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "stsb-roberta-base-v2\n",
      "80.7 ms ± 3.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "nli-distilroberta-base-v2\n",
      "32.3 ms ± 1.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "average_word_embeddings_komninos\n",
      "564 µs ± 95.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "average_word_embeddings_glove.6B.300d\n",
      "596 µs ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# large action set\n",
    "embeddings = []\n",
    "\n",
    "for i, bert_model in enumerate(bert_models):\n",
    "    print(models[i])\n",
    "    %timeit bert_model.embed(data.utterances[0])\n",
    "    embeddings.append(bert_model.embed(data.utterances))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63d2649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of utterances: 155\n",
      "number of pairs: 24025\n",
      "number of same: 455\n",
      "number of different: 23570\n"
     ]
    }
   ],
   "source": [
    "# create data pairs\n",
    "indices = range(len(data.utterances))\n",
    "all_pairs = [(i, j) for i in indices for j in indices]\n",
    "print(\"number of utterances:\", len(data.utterances))\n",
    "print(\"number of pairs:\", len(all_pairs))\n",
    "\n",
    "true_values = [] # 1 is for same, 0 is different action\n",
    "for i1, i2 in all_pairs:\n",
    "    true_values.append( int(data.actions[i1] == data.actions[i2]) )\n",
    "    \n",
    "true_values = np.array(true_values)\n",
    "print(\"number of same:\", len([x for x in true_values if x == 1]))\n",
    "print(\"number of different:\", len([x for x in true_values if x == 0]))\n",
    "\n",
    "def calc_performance_score(embeddings, sim):\n",
    "    # first calculate similarities\n",
    "    similarities = []\n",
    "    for i1, i2 in all_pairs:\n",
    "        similarities.append(sim.calculate(embeddings[i1], embeddings[i2]))\n",
    "    similarities = np.array(similarities)\n",
    "    \n",
    "    same = list(itertools.compress(similarities, true_values))\n",
    "    different = list(itertools.compress(similarities, 1 - true_values))\n",
    "\n",
    "    return (same, different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35240c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(same, diff):\n",
    "    plt.boxplot([same, diff], [1, 2])\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks([1, 2], [\"same\", \"different\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d55cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_latex_table_entry(name, performance, overall, same, different):\n",
    "    print(f\"{name} & {performance:2.2f} & {100*overall:2.2f} & {100*same:2.2f} & {100*different:2.2f} \\\\\\\\\")\n",
    "    print(\"\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c3acc",
   "metadata": {},
   "source": [
    "first number is overall performance (average of: mean of similarities for same actions & 1 - mean of similarities for different actions)\n",
    "\n",
    "first number in parantheses indicates the mean of the similarities for utterances that map to the same action (higher is better)\n",
    "\n",
    "second number in parantheses is the mean of similarities for utterances that map to different actions (lower is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c505462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for Cosine Similarity\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\BachelorThesis\\pandas_nli\\nl2pandas\\backend\\nli_for_pandas\\similarity\\cosine_similarity.py:20: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  return np.dot(vector1, vector2) / (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7346 (0.8093 | 0.3402)    - paraphrase-mpnet-base-v2\n",
      "0.7596 (0.7658 | 0.2466)    - paraphrase-TinyBERT-L6-v2\n",
      "0.7527 (0.7847 | 0.2794)    - paraphrase-distilroberta-base-v2\n",
      "0.7447 (0.7837 | 0.2943)    - paraphrase-MiniLM-L12-v2\n",
      "0.7454 (0.7694 | 0.2786)    - paraphrase-MiniLM-L6-v2\n",
      "0.7445 (0.7519 | 0.2630)    - paraphrase-albert-small-v2\n",
      "0.7462 (0.7818 | 0.2894)    - paraphrase-MiniLM-L3-v2\n",
      "0.7075 (0.8039 | 0.3890)    - nli-mpnet-base-v2\n",
      "0.7225 (0.8019 | 0.3568)    - stsb-mpnet-base-v2\n",
      "0.7144 (0.7654 | 0.3365)    - stsb-distilroberta-base-v2\n",
      "0.6840 (0.8284 | 0.4605)    - nli-roberta-base-v2\n",
      "0.7197 (0.7670 | 0.3275)    - stsb-roberta-base-v2\n",
      "0.6901 (0.8162 | 0.4359)    - nli-distilroberta-base-v2\n",
      "nan (nan | nan)    - average_word_embeddings_komninos\n",
      "nan (nan | nan)    - average_word_embeddings_glove.6B.300d\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance for Cosine Similarity\")\n",
    "print()\n",
    "cosine_sim = CosineSimilarity()\n",
    "performances = [calc_performance_score(embedding, cosine_sim) for embedding in embeddings]\n",
    "for i, (same, diff) in enumerate(performances):\n",
    "    p_same = np.mean(same)\n",
    "    p_diff = np.mean(diff)\n",
    "    performance = (p_same + (1 - p_diff)) / 2\n",
    "    print(f\"{performance:.4f} ({p_same:.4f} | {p_diff:.4f})    - {models[i]}\")\n",
    "    # visualize(same, diff)\n",
    "    # print_latex_table_entry(models[i], overall[i], performance, p_same, p_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ee77ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paraphrase-mpnet-base-v2\n",
      "47.5 ms ± 1.31 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-TinyBERT-L6-v2\n",
      "24.4 ms ± 336 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-distilroberta-base-v2\n",
      "29.8 ms ± 2.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-MiniLM-L12-v2\n",
      "20.5 ms ± 288 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-MiniLM-L6-v2\n",
      "17.2 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "paraphrase-albert-small-v2\n",
      "38.9 ms ± 1.03 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "paraphrase-MiniLM-L3-v2\n",
      "9.7 ms ± 253 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "nli-mpnet-base-v2\n",
      "75.4 ms ± 809 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "stsb-mpnet-base-v2\n",
      "76 ms ± 1.11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "stsb-distilroberta-base-v2\n",
      "30.5 ms ± 2.49 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "nli-roberta-base-v2\n",
      "60 ms ± 3.87 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "stsb-roberta-base-v2\n",
      "74.7 ms ± 1.47 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "nli-distilroberta-base-v2\n",
      "37.9 ms ± 3.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# small action set\n",
    "embeddings = []\n",
    "\n",
    "for i, bert_model in enumerate(bert_models):\n",
    "    print(models[i])\n",
    "    %timeit bert_model.embed(data_small.utterances[0])\n",
    "    embeddings.append(bert_model.embed(data_small.utterances))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9514a7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of utterances: 76\n",
      "number of pairs: 5776\n",
      "number of same: 216\n",
      "number of different: 5560\n"
     ]
    }
   ],
   "source": [
    "# create data pairs\n",
    "indices = range(len(data_small.utterances))\n",
    "all_pairs = [(i, j) for i in indices for j in indices]\n",
    "print(\"number of utterances:\", len(data_small.utterances))\n",
    "print(\"number of pairs:\", len(all_pairs))\n",
    "\n",
    "true_values = [] # 1 is for same, 0 is different action\n",
    "for i1, i2 in all_pairs:\n",
    "    true_values.append( int(data_small.actions[i1] == data_small.actions[i2]) )\n",
    "    \n",
    "true_values = np.array(true_values)\n",
    "print(\"number of same:\", len([x for x in true_values if x == 1]))\n",
    "print(\"number of different:\", len([x for x in true_values if x == 0]))\n",
    "\n",
    "def calc_performance_score(embeddings, sim):\n",
    "    # first calculate similarities\n",
    "    similarities = []\n",
    "    for i1, i2 in all_pairs:\n",
    "        similarities.append(sim.calculate(embeddings[i1], embeddings[i2]))\n",
    "    similarities = np.array(similarities)\n",
    "    \n",
    "    same = list(itertools.compress(similarities, true_values))\n",
    "    different = list(itertools.compress(similarities, 1 - true_values))\n",
    "\n",
    "    return (same, different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "471ba098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for Cosine Similarity\n",
      "\n",
      "0.7425 (0.8054 | 0.3204)    - paraphrase-mpnet-base-v2\n",
      "0.7612 (0.7640 | 0.2415)    - paraphrase-TinyBERT-L6-v2\n",
      "0.7655 (0.7836 | 0.2527)    - paraphrase-distilroberta-base-v2\n",
      "0.7532 (0.7781 | 0.2718)    - paraphrase-MiniLM-L12-v2\n",
      "0.7520 (0.7673 | 0.2632)    - paraphrase-MiniLM-L6-v2\n",
      "0.7539 (0.7477 | 0.2399)    - paraphrase-albert-small-v2\n",
      "0.7609 (0.7711 | 0.2494)    - paraphrase-MiniLM-L3-v2\n",
      "0.7271 (0.7951 | 0.3410)    - nli-mpnet-base-v2\n",
      "0.7423 (0.8030 | 0.3184)    - stsb-mpnet-base-v2\n",
      "0.7321 (0.7646 | 0.3004)    - stsb-distilroberta-base-v2\n",
      "0.7066 (0.8223 | 0.4090)    - nli-roberta-base-v2\n",
      "0.7377 (0.7653 | 0.2899)    - stsb-roberta-base-v2\n",
      "0.7130 (0.8124 | 0.3865)    - nli-distilroberta-base-v2\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance for Cosine Similarity\")\n",
    "print()\n",
    "cosine_sim = CosineSimilarity()\n",
    "performances = [calc_performance_score(embedding, cosine_sim) for embedding in embeddings]\n",
    "for i, (same, diff) in enumerate(performances):\n",
    "    p_same = np.mean(same)\n",
    "    p_diff = np.mean(diff)\n",
    "    performance = (p_same + (1 - p_diff)) / 2\n",
    "    print(f\"{performance:.4f} ({p_same:.4f} | {p_diff:.4f})    - {models[i]}\")\n",
    "    # visualize(same, diff)\n",
    "    # print_latex_table_entry(models[i], overall[i], performance, p_same, p_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa396f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
